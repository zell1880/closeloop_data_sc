{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Malay Stemmer** \n",
    "stolen from https://github.com/huseinzol05/Malaya/blob/master/session/emotion/multinomial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "permulaan = [\n",
    "    'bel',\n",
    "    'se',\n",
    "    'ter',\n",
    "    'men',\n",
    "    'meng',\n",
    "    'mem',\n",
    "    'memper',\n",
    "    'di',\n",
    "    'pe',\n",
    "    'me',\n",
    "    'ke',\n",
    "    'ber',\n",
    "    'pen',\n",
    "    'per',\n",
    "]\n",
    "\n",
    "hujung = ['kan', 'kah', 'lah', 'tah', 'nya', 'an', 'wan', 'wati', 'ita']\n",
    "\n",
    "def naive_stemmer(word):\n",
    "    assert isinstance(word, str), 'input must be a string'\n",
    "    hujung_result = re.findall(r'^(.*?)(%s)$' % ('|'.join(hujung)), word)\n",
    "    word = hujung_result[0][0] if len(hujung_result) else word\n",
    "    permulaan_result = re.findall(r'^(.*?)(%s)' % ('|'.join(permulaan[::-1])), word)\n",
    "    permulaan_result.extend(re.findall(r'^(.*?)(%s)' % ('|'.join(permulaan)), word))\n",
    "    mula = permulaan_result if len(permulaan_result) else ''\n",
    "    if len(mula):\n",
    "        mula = mula[1][1] if len(mula[1][1]) > len(mula[0][1]) else mula[0][1]\n",
    "    return word.replace(mula, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File import**\n",
    "note that I change Sarcasm_Headlines_Dataset.json to Sarcasm_Headlines_Dataset2.json due to some f ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'bm-sarcastic-news-headline.json', 'Sarcasm Detection English.ipynb', 'Sarcasm_Headlines_Dataset - Copy.json', 'Sarcasm_Headlines_Dataset.json', 'Sarcasm_Headlines_Dataset2.json', 'Untitled.ipynb', '__MACOSX']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../Sarcasm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = pd.read_json('../Sarcasm/Sarcasm_Headlines_Dataset2.json',lines=True)\n",
    "dataset_my = pd.read_json('../Sarcasm/bm-sarcastic-news-headline.json',lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting and Rejoining Malay Headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_sar=pd.DataFrame({'headline':dataset_my.iloc[0,1],'is_sarcastic':1})\n",
    "my_nsar= pd.DataFrame({'headline':dataset_my.iloc[0,0],'is_sarcastic':0})\n",
    "data_my = my_sar.append(my_nsar, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excluding Website in English Headlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = dataset_en.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports for Stopwords, Tokenizer and Lemmatizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords, Tokenizer and Lemmatizer for English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_en = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenizer_nltk = RegexpTokenizer('[0-9]*\\.[0-9]*|\\w+')\n",
    "for i in range(len(data_en)):\n",
    "    tokens = tokenizer_nltk.tokenize(data_en.iloc[i,0])\n",
    "    tokens = [lemmatizer.lemmatize(x)for x in tokens]\n",
    "    tok = [x for x in tokens if ((x not in stop_en) and (x not in string.punctuation))]\n",
    "    data_en.iloc[i,0] = ' '.join(tok)\n",
    "data_en_clean = data_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopword from Malay Language**\n",
    "\n",
    "Words obtain from:\n",
    "https://blog.kerul.net/2014/01/list-of-malay-stop-words.html\n",
    "List contains words with -lah,-kah etc, which is redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_my = ['ada','inikah','sampai','adakah','inilah','sana',\n",
    "        'adakan','itu','sangat','adalah','itukah','sangatlah',\n",
    "        'adanya','itulah','saya','adapun','jadi','se','agak',\n",
    "        'jangan','seandainya','agar','janganlah','sebab','akan',\n",
    "        'jika','sebagai','aku','jikalau','sebagaimana','akulah',\n",
    "        'jua','sebanyak','akupun','juapun','sebelum','al','juga',\n",
    "        'sebelummu','alangkah','kalau','sebelumnya','allah',\n",
    "        'kami','sebenarnya','amat','kamikah','secara','antara',\n",
    "        'kamipun','sedang','antaramu','kamu','edangkan',\n",
    "        'antaranya','amukah','edikit','apa','kamupun',\n",
    "        'sedikitpun','apa-apa','katakan','segala','apabila',\n",
    "        'ke','sehingga','apakah','kecuali','sejak','apapun',\n",
    "        'kelak','sekalian','atas','kembali','sekalipun','atasmu',\n",
    "        'kemudian','sekarang','atasnya','kepada','sekitar',\n",
    "        'atau','kepadaku','selain','ataukah','kepadakulah',\n",
    "        'selalu','ataupun','kepadamu','selama','bagaimana',\n",
    "        'kepadanya','selama-lamanya','bagaimanakah',\n",
    "        'kepadanyalah','seluruh','bagi','kerana','seluruhnya',\n",
    "        'bagimu','kerananya','sementara','baginya','kesan',\n",
    "        'semua','bahawa','ketika','semuanya','bahawasanya',\n",
    "        'kini','semula','bahkan','kita','senantiasa','bahwa',\n",
    "        'ku','sendiri','banyak','kurang','sentiasa','banyaknya',\n",
    "        'lagi','seolah','barangsiapa','lain','seolah-olah',\n",
    "        'bawah','lalu','seorangpun','beberapa','lamanya',\n",
    "        'separuh','begitu','langsung','sepatutnya','begitupun',\n",
    "        'lebih','seperti','belaka','maha','seraya','belum','mahu',\n",
    "        'sering','belumkah','mahukah','serta','berada',\n",
    "        'mahupun','seseorang','berapa','maka','sesiapa','berikan',\n",
    "        'malah','sesuatu','beriman','mana','sesudah','berkenaan',\n",
    "        'manakah','sesudahnya','berupa','manapun','sesungguhnya',\n",
    "        'beserta','masih','sesungguhnyakah','biarpun','masing',\n",
    "        'setelah','bila','masing-masing','setiap','bilakah',\n",
    "        'melainkan','siapa','bilamana','memang','siapakah',\n",
    "        'bisa','mempunyai','sini','boleh','mendapat','situ',\n",
    "        'bukan','mendapati','situlah','bukankah','mendapatkan',\n",
    "        'suatu','bukanlah','mengadakan','sudah','dahulu',\n",
    "        'mengapa','sudahkah','dalam','mengapakah','sungguh',\n",
    "        'dalamnya','mengenai','sungguhpun','dan','menjadi',\n",
    "        'supaya','dapat','menyebabkan','tadinya','dapati',\n",
    "        'menyebabkannya','tahukah','dapatkah','mereka','tak',\n",
    "        'dapatlah','merekalah','tanpa','dari','merekapun',\n",
    "        'tanya','daripada','meskipun','tanyakanlah','daripadaku',\n",
    "        'mu','tapi','daripadamu','nescaya','telah','daripadanya',\n",
    "        'niscaya','tentang','demi','nya','tentu','demikian',\n",
    "        'olah','terdapat','demikianlah','oleh','terhadap',\n",
    "        'dengan','orang','terhadapmu','dengannya','pada',\n",
    "        'termasuk','di','padahal','terpaksa','dia','padamu',\n",
    "        'tertentu','dialah','padanya','tetapi','didapat',\n",
    "        'paling','tiada','didapati','para','tiadakah',\n",
    "        'dimanakah','pasti','tiadalah','engkau','patut',\n",
    "        'tiap','engkaukah','patutkah','tiap-tiap','engkaulah',\n",
    "        'per','tidak','engkaupun','pergilah','tidakkah','hai',\n",
    "        'perkara','tidaklah','hampir','perkaranya','turut',\n",
    "        'hampir-hampir','perlu','untuk','hanya','pernah',\n",
    "        'untukmu','anyalah','pertama','wahai','hendak','pula',\n",
    "        'walau','hendaklah','pun','walaupun','hingga','sahaja',\n",
    "        'ya','ia','saja','yaini','iaitu','saling','yaitu',\n",
    "        'ialah','sama','yakni','ianya','sama-sama','yang',\n",
    "        'inginkah','samakah','ini','sambil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing, Stemming and Removing StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_nltk = RegexpTokenizer('[0-9]*\\.[0-9]*|\\w+')\n",
    "for i in range(len(data_my)):\n",
    "    tokens = tokenizer_nltk.tokenize(data_my.iloc[i,0])\n",
    "    tokens = [naive_stemmer(x) for x in tokens]\n",
    "    tok = [x for x in tokens if ((x not in stop_my) and (x not in string.punctuation))]\n",
    "    data_my.iloc[i,0] = ' '.join(tok)\n",
    "data_my_clean = data_my   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3),min_df=2).fit(data_en_clean['headline'])\n",
    "en_vectors = tfidf.transform(data_en_clean['headline'])\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3),min_df=2).fit(data_my_clean['headline'])\n",
    "my_vectors = tfidf.transform(data_my_clean['headline'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this to put Malay Dataset into Test and Training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(my_vectors, data_my_clean['is_sarcastic'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this to put English Dataset into Test and Training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(en_vectors, data_en_clean['is_sarcastic'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting Data into model and predicting Test Data**\n",
    "\n",
    "no idea which model to use, followed https://github.com/huseinzol05/Malaya/blob/master/session/emotion/multinomial.ipynb\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " is_sarcastic       0.88      0.96      0.92     12011\n",
      "not_sarcastic       0.95      0.83      0.89      9356\n",
      "\n",
      "    micro avg       0.91      0.91      0.91     21367\n",
      "    macro avg       0.91      0.90      0.90     21367\n",
      " weighted avg       0.91      0.91      0.91     21367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model  = MultinomialNB().fit(train_X, train_Y)\n",
    "print(\n",
    "    metrics.classification_report(\n",
    "        train_Y,\n",
    "        model.predict(train_X),\n",
    "        target_names = ['is_sarcastic','not_sarcastic'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      " is_sarcastic       0.79      0.90      0.84      2974\n",
      "not_sarcastic       0.85      0.69      0.76      2368\n",
      "\n",
      "    micro avg       0.81      0.81      0.81      5342\n",
      "    macro avg       0.82      0.80      0.80      5342\n",
      " weighted avg       0.81      0.81      0.81      5342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    metrics.classification_report(\n",
    "        test_Y,\n",
    "        model.predict(test_X),\n",
    "        target_names = ['is_sarcastic','not_sarcastic'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Keras Stuff I have been playing around and trying to figure out**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_en\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(dataset)):\n",
    "    X.append(data.iloc[i,0])\n",
    "    Y.append([data.iloc[i,1]])\n",
    "y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train, Xs_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state =42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(Xs_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(Xs_train)\n",
    "X_test = tokenizer.texts_to_sequences(Xs_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20097\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annoying ad turn man pro whaling\n",
      "[2271, 288, 255, 3, 767, 11099]\n"
     ]
    }
   ],
   "source": [
    "print(Xs_train[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 32\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen = maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen= maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "from keras.layers import Conv1D,GlobalMaxPool1D,MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My Trash Model**\n",
    "\n",
    "Had no idea what those layers are, might as well try it out\n",
    "\n",
    "Eeek 50% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(32,),activation ='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,activation ='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation ='sigmoid'))\n",
    "model.compile(optimizer = keras.optimizers.Nadam(),\n",
    "             loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some guy's model I stole in Kaggle**\n",
    "\n",
    "99% accuracy, wow\n",
    "\n",
    "source:https://www.kaggle.com/vrishabhps/sarcasm-detection-using-keras-and-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim,input_length=maxlen))\n",
    "model.add(layers.LSTM(embedding_dim, return_sequences = True))\n",
    "model.add(layers.Conv1D(128,3,activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10,activation='relu'))\n",
    "model.add(layers.Dense(1,activation ='sigmoid'))\n",
    "model.compile(optimizer = keras.optimizers.Nadam(),\n",
    "             loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21367 samples, validate on 5342 samples\n",
      "Epoch 1/5\n",
      " - 259s - loss: 0.0136 - acc: 0.9952 - val_loss: 1.2838 - val_acc: 0.7900\n",
      "Epoch 2/5\n",
      " - 246s - loss: 0.0096 - acc: 0.9965 - val_loss: 1.3408 - val_acc: 0.7879\n",
      "Epoch 3/5\n",
      " - 245s - loss: 0.0088 - acc: 0.9971 - val_loss: 1.7028 - val_acc: 0.7782\n",
      "Epoch 4/5\n",
      " - 247s - loss: 0.0060 - acc: 0.9981 - val_loss: 1.4560 - val_acc: 0.7870\n",
      "Epoch 5/5\n",
      " - 253s - loss: 0.0067 - acc: 0.9978 - val_loss: 1.3041 - val_acc: 0.7812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dcf71b8eb8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test), batch_size=10,epochs=5,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
